{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f88a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ['GROQ_API_KEY'] ='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'\n",
    "llm = init_chat_model('groq:llama-3.1-8b-instant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03877322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start->router->general->respond\n",
    "#         ->more info->        |->             |->checkHallucination->end\n",
    "#         ->createPlan->conduct research->respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a84d505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph,END,START\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict,Annotated,Union,List\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77f0c539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    researchPlan: str\n",
    "    additionalInfo: str\n",
    "    researchResults: str\n",
    "    webSearch: str\n",
    "    retrievedDocuments: str\n",
    "    response: str\n",
    "    nextNode: str\n",
    "    chatHistory: Annotated[List[HumanMessage],MessagesPlaceholder('chatHistory')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e54a5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model('groq:llama-3.1-8b-instant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ecb79efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysePrompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"\n",
    "    You are a great analyser that analyses the user queries for their category.The user query might be multiple sentence with sometimes clarification with them, consider them as 1 query. Analyse the user query and determine the apporpriate action:\n",
    "    \n",
    "    1.If the query is too vague, cannot be used to generate precise answer and require some context and clarification from user , respond: 'CLARIFY'\n",
    "    2.If it is general information related query and not requires much research and you have enough input from user, respond:'GENERAl'.\n",
    "    3.If it requires researching for information and analysis , respond: 'RESEARCH'.\n",
    "    \n",
    "    EXAMPLE\n",
    "    'What is the weather today?' respond:'CLARIFY'\n",
    "    'Help me with my homework.' respond:'CLARIFY'\n",
    "    'Help me with my homework. what is 2+2' respond:'GENERAL'\n",
    "    'What is the capital of france?' respond:'GENERAL'\n",
    "    'Why does it snow?' respond:'GENERAL'\n",
    "    'Write a report in quantum computing.' respond:'RESEARCH'\n",
    "    'Progress in deep learning over the years.' respond:'RESEARCH'\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder('chatHistory'),\n",
    "    (\"human\",\"{input}\"+\". \"+\"{additionalInfo}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07e34b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalPrompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"Answer the user question precisely and clearly.\"\"\"),\n",
    "    MessagesPlaceholder(\"chatHistory\"),\n",
    "    (\"human\",\"{input}\"+\". \"+\"{additionalInfo}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eda7b55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moreInfoPrompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"Ask Clarifying question to better understand what user needs.Maximum questions is 3.\n",
    "    Your output must only be a JSON object containing a single key 'queries' nothing else at all:\n",
    "    {{ 'queries': ['Question 1', 'Question 2',...] }}\"\"\"),\n",
    "    MessagesPlaceholder(\"chatHistory\"),\n",
    "    (\"human\",\"{input}\"+\". \"+\"{additionalInfo}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "027ee346",
   "metadata": {},
   "outputs": [],
   "source": [
    "createPlan = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"Create a detailed research plan to answer the user queries.\"\"\"),\n",
    "    MessagesPlaceholder('chatHistory'),\n",
    "    (\"human\",\"{input}\"+\". \"+\"{additionalInfo}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55d4880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conductResearch = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\",\"\"\"Conduct research based on this research plan:{researchPlan}\"\"\"),\n",
    "#     MessagesPlaceholder('chatHistory'),\n",
    "#     (\"human\",\"{input}\"+\". \"+\"{additionalInfo}\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9679e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state:AgentState):\n",
    "    chain = analysePrompt | llm\n",
    "    result = chain.invoke({\n",
    "        \"input\":state['input'],\n",
    "        \"chatHistory\": state['chatHistory'],\n",
    "        \"additionalInfo\":state['additionalInfo']\n",
    "    })\n",
    "    decision = result.content.lower()\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"DECISION=\",decision)\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    if \"general\" in decision:\n",
    "        print(\"GENERAL\")\n",
    "        return {\"nextNode\":\"general\"}\n",
    "    \n",
    "    elif \"clarify\" in decision:\n",
    "        print(\"MOREINFO\")\n",
    "        return {\"nextNode\":\"moreInfo\"}\n",
    "\n",
    "    else:\n",
    "        print(\"RESEARCH\")\n",
    "        return {\"nextNode\":\"createResearchPlan\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4f9159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def researchPlan(state:AgentState):\n",
    "    \n",
    "    chain = createPlan |llm\n",
    "    result = chain.invoke({\n",
    "        \"input\":state['input'],\n",
    "        \"chatHistory\":state['chatHistory'],\n",
    "        \"additionalInfo\":state['additionalInfo']\n",
    "    })\n",
    "    return {\"researchPlan\":result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "702e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conductResearch(state:AgentState):\n",
    "#     chain = conductResearch | llm\n",
    "#     result = chain.invoke({\n",
    "#         \"input\":state['input'],\n",
    "#         \"chatHistory\":state['chatHistory'],\n",
    "#         \"researchPlan\":state['researchPlan'],\n",
    "#         \"additionalInfo\":state['additionalInfo']\n",
    "#     })\n",
    "#     return {'researchResult':result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325a911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6fbd954",
   "metadata": {},
   "source": [
    "### Research Sub-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bed24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa08c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a823464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import add_messages\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    query: str\n",
    "    messages: Annotated[list,add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02a5b447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def Retrieve(query: str):\n",
    "    \"\"\"retrieve the arxiv for papers related to query\"\"\"\n",
    "    all_papers=[]\n",
    "    \n",
    "    retriever = ArxivRetriever(\n",
    "        load_max_docs=2,\n",
    "        get_ful_documents=True,\n",
    "    )\n",
    "#     for query in nquery:\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    for doc in docs:\n",
    "        paper_info = {\n",
    "            \"title\": doc.metadata['Title'],\n",
    "            \"summary\": doc.page_content,\n",
    "            \"authors\": [author for author in doc.metadata['Authors'].split(',')],\n",
    "            \"published\": doc.metadata['Published'].strftime(\"%Y-%m-%d\"),\n",
    "            \"url\": doc.metadata['Entry ID']\n",
    "        }\n",
    "        all_papers.append(paper_info)\n",
    "        #limiting to one paper\n",
    "        break\n",
    "\n",
    "\n",
    "    return \"Summary: \"+all_papers[0]['summary']+\"\\n\\n Source: \"+all_papers[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "519e5216",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Action parsing in videos with complex scenes is an interesting but\n",
      "challenging task in computer vision. In this paper, we propose a generic 3D\n",
      "convolutional neural network in a multi-task learning manner for effective Deep\n",
      "Action Parsing (DAP3D-Net) in videos. Particularly, in the training phase,\n",
      "action localization, classification and attributes learning can be jointly\n",
      "optimized on our appearancemotion data via DAP3D-Net. For an upcoming test\n",
      "video, we can describe each individual action in the video simultaneously as:\n",
      "Where the action occurs, What the action is and How the action is performed. To\n",
      "well demonstrate the effectiveness of the proposed DAP3D-Net, we also\n",
      "contribute a new Numerous-category Aligned Synthetic Action dataset, i.e.,\n",
      "NASA, which consists of 200; 000 action clips of more than 300 categories and\n",
      "with 33 pre-defined action attributes in two hierarchical levels (i.e.,\n",
      "low-level attributes of basic body part movements and high-level attributes\n",
      "related to action motion). We learn DAP3D-Net using the NASA dataset and then\n",
      "evaluate it on our collected Human Action Understanding (HAU) dataset.\n",
      "Experimental results show that our approach can accurately localize, categorize\n",
      "and describe multiple actions in realistic videos.\n",
      "\n",
      " Source: http://arxiv.org/abs/1602.03346v1\n"
     ]
    }
   ],
   "source": [
    "res = Retrieve(\"action classification using deep learning\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccdaffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb6c207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "tavily_client = TavilyClient(api_key=\"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\")\n",
    "\n",
    "@tool\n",
    "def WebSearch(query:str):\n",
    "    \"\"\"search the web for information related to query\"\"\"\n",
    "    \n",
    "    result = tavily_client.search(query,max_results=2)\n",
    "    response = []\n",
    "    \n",
    "    for i in range(len(result['results'])):\n",
    "        response.append(result['results'][i]['content'])\n",
    "    return response[0]+\"\\n\\n\"+response[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53eb4f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Web oficial Leo Messi jugador del Inter de Miami – messi.com – Web oficial de Lionel Messi, jugador del Futbol Profesional y campeón mundial con la selección Argentina Web oficial Leo Messi jugador del Inter de Miami - messi.com Inter Miami CF disputa este domingo la novena jornada de la MLS ante Dallas en el Chase Stadium. Leo se enfrenta este sábado a Columbus Crew en la octava jornada de la MLS para Inter Miami en el […] Inter Miami y Chicago Fire empataron 0-0 en el partido correspondiente a la jornada 7 de la MLS del encuentro […] Leo Messi Management S.L.U. utiliza cookies propias y de terceros para ofrecerle contenidos adaptados a sus intereses.\\n\\nWidely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and eight times being named the world's best player by FIFA.[note 2] He is the most decorated player in the history of professional football having won 45 team trophies,[note 3] including twelve Big Five league titles, four UEFA Champions Leagues, two Copa Américas, and one FIFA World Cup. Messi holds the records for most European Golden Shoes (6), most goals in a calendar year (91), most goals for a single club (672, with Barcelona), most goals (474), hat-tricks (36) and assists (192) in La Liga, most assists (18) and goal contributions (32) in the Copa América, most goal contributions (21) in the World Cup, most international appearances (191) and international goals (112) by a South American male, and the second-most in the latter category outright.\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = WebSearch(\"who is leo messi?\")\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1f1dd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and eight times being named the world's best player by FIFA.[note 2] He is the most decorated player in the history of professional football having won 45 team trophies,[note 3] including twelve Big Five league titles, four UEFA Champions Leagues, two Copa Américas, and one FIFA World Cup. Messi holds the records for most European Golden Shoes (6), most goals in a calendar year (91), most goals for a single club (672, with Barcelona), most goals (474), hat-tricks (36) and assists (192) in La Liga, most assists (18) and goal contributions (32) in the Copa América, most goal contributions (21) in the World Cup, most international appearances (191) and international goals (112) by a South American male, and the second-most in the latter category outright.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(response['results'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0409770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[WebSearch, Retrieve]\n",
    "\n",
    "agent_system_prompt = \"\"\"\n",
    "< Role >\n",
    "You are research assistant. You are a top-notch research assistant.Your task is to do research. \n",
    "</ Role >\n",
    "\n",
    "< Tools >\n",
    "You have access to the following tools:\n",
    "\n",
    "1. WebSearch(query): search the web for information related to query\n",
    "2. Retrieve(query): retrieve the arxiv for papers related to query\n",
    "\n",
    "</ Tools >\n",
    "\n",
    "< Instructions >\n",
    "Using the tools available to gather information about the topic.\n",
    "Do not return anything after generating last output.\n",
    "Return last output as: {\"webSearch\":web_searched_data_here_exactly_as_retieved,\"retrieve\":arxiv_retireved_data_here_with_summary_and_source_exactly_as_retieved}\n",
    "</ Instructions >\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ed4c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(state):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": agent_system_prompt\n",
    "                \n",
    "        }\n",
    "    ] + state['messages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fb85a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subagent = create_react_agent(\n",
    "    'groq:llama-3.1-8b-instant',\n",
    "    tools=tools,\n",
    "    prompt=create_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181fdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "eefc1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = subagent.invoke(\n",
    "    {\"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Classification of user action using machine learning\"\n",
    "    }]},\n",
    "#     config = RunnableConfig(recursion_limit=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "32d13538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"webSearch\": \"Online classification of user activities using machine learning on network traffic - ScienceDirect\",\"retrieve\": \"The current research has aimed to investigate and develop machine-learning approaches by using the data in the dataset to be applied to classify location-based social network data and predict user activities based on the nature of various locations (such as entertainment). The analysis of user activities and behavior from location-based social network data is often based on venue types, which\\\\nSummary: Machine learning has shown successes for complex learning problems in which\\\\ndata/parameters can be multidimensional and too complex for a first-principles\\\\nbased analysis. Some applications that utilize machine learning require human\\\\ninterpretability, not just to understand a particular result (classification,\\\\ndetection, etc.) but also for humans to take action based on that result.\\\\nBlack-box machine learning model interpretation has been studied, but recent\\\\nwork has focused on validation and improving model performance. In this work,\\\\nan actionable interpretation of black-box machine learning models is presented.\\\\nThe proposed technique focuses on the extraction of actionable measures to help\\\\nusers make a decision or take an action. Actionable interpretation can be\\\\nimplemented in most traditional black-box machine learning models. It uses the\\\\nalready trained model, used training data, and data processing techniques to\\\\nextract actionable items from the model outcome and its time-series inputs. An\\\\nimplementation of the actionable interpretation is shown with a use case:\\\\ndementia-related agitation prediction and the ambient environment. It is shown\\\\nthat actionable items can be extracted, such as the decreasing of in-home light\\\\nlevel, which is triggering an agitation episode. This use case of actionable\\\\ninterpretation can help dementia caregivers take action to intervene and\\\\nprevent agitation.\\\\n Source: http://arxiv.org/abs/2009.05097v1\"}'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "da380e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'webSearch': 'Online classification of user activities using machine learning on network traffic - ScienceDirect',\n",
       " 'retrieve': 'The current research has aimed to investigate and develop machine-learning approaches by using the data in the dataset to be applied to classify location-based social network data and predict user activities based on the nature of various locations (such as entertainment). The analysis of user activities and behavior from location-based social network data is often based on venue types, which\\nSummary: Machine learning has shown successes for complex learning problems in which\\ndata/parameters can be multidimensional and too complex for a first-principles\\nbased analysis. Some applications that utilize machine learning require human\\ninterpretability, not just to understand a particular result (classification,\\ndetection, etc.) but also for humans to take action based on that result.\\nBlack-box machine learning model interpretation has been studied, but recent\\nwork has focused on validation and improving model performance. In this work,\\nan actionable interpretation of black-box machine learning models is presented.\\nThe proposed technique focuses on the extraction of actionable measures to help\\nusers make a decision or take an action. Actionable interpretation can be\\nimplemented in most traditional black-box machine learning models. It uses the\\nalready trained model, used training data, and data processing techniques to\\nextract actionable items from the model outcome and its time-series inputs. An\\nimplementation of the actionable interpretation is shown with a use case:\\ndementia-related agitation prediction and the ambient environment. It is shown\\nthat actionable items can be extracted, such as the decreasing of in-home light\\nlevel, which is triggering an agitation episode. This use case of actionable\\ninterpretation can help dementia caregivers take action to intervene and\\nprevent agitation.\\n Source: http://arxiv.org/abs/2009.05097v1'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "input_str = json.loads(response['messages'][-1].content)\n",
    "input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3a881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e8aeb3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # from langgraph.graph import StateGraph, START, END\n",
    "# # from langgraph.types import Command\n",
    "# # from typing import Literal\n",
    "# # from IPython.display import Image, display\n",
    "# research = StateGraph(ResearchState)\n",
    "# research.add_node(\"response_agent\", subagent)\n",
    "# # research.add_edge(START, \"response_agent\")\n",
    "# research.set_entry_point(\"response_agent\")\n",
    "# research.add_edge(\"response_agent\",END)\n",
    "# research = research.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1528a0fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(Image(\u001b[43mresearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/langchain_core/runnables/graph.py:685\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_mermaid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    679\u001b[0m mermaid_syntax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    680\u001b[0m     curve_style\u001b[38;5;241m=\u001b[39mcurve_style,\n\u001b[1;32m    681\u001b[0m     node_colors\u001b[38;5;241m=\u001b[39mnode_colors,\n\u001b[1;32m    682\u001b[0m     wrap_label_n_words\u001b[38;5;241m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    683\u001b[0m     frontmatter_config\u001b[38;5;241m=\u001b[39mfrontmatter_config,\n\u001b[1;32m    684\u001b[0m )\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    287\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    288\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    289\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[0;32m--> 293\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     supported_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m requests\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    432\u001b[0m             img_bytes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/urllib3/connectionpool.py:463\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# display(Image(research.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd8697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9071d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = research.invoke(\n",
    "    {\"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"computer vision\"\n",
    "    }]},\n",
    "#     config = RunnableConfig(recursion_limit=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b856c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"webSearch\":\"Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4] \\\\\"Understanding\\\\\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance[26] in industrial applications.[22] Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking[27]).\", \"retrieve\":\"Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise. This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications. Source: http://arxiv.org/abs/1905.07844v1\"}'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df4ccbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"webSearch\": \"Quantum Computing and its Applications\", \"retrieve\": \"Application of Quantum Computing What is Quantum Computing? How Quantum Computing is Different from Classical Computing? Quantum computing uses qubits to run quantum algorithms. The processing power of quantum computers can increase exponentially if more qubits are added. Like the processing power of quantum computers can increase exponentially if more qubits are added, similarly the processing power of classical computers increases linearly when more bits are added. Current State of Quantum Computing The field of quantum computing is evolving. Application of Quantum Computing Quantum computing can be used in various fields such as: Quantum Computing in Drug Discovery Quantum Computing in Cybersecurity Quantum Computing in Logistics Quantum Computing in Manufacturing Challenges and Limitations of Quantum Computing Future Prospects of Quantum Computing\\\\nThe discovery by Microsoft and Quantinuum addresses this problem and reignites the heated race between top tech companies like Microsoft, Google and IBM to conquer quantum computing. 10 Companies Utilizing Quantum Computing IBM’s research came in the wake of another promising machine-learning classification algorithm: a quantum-classical hybrid run on a 19-qubit machine built by\\xa0Rigetti Computing. “Harnessing [quantum computers’ statistical distribution] has the potential to accelerate or otherwise improve machine learning relative to purely classical performance,”\\xa0Rigetti researchers wrote. It partners with quantum-computing leaders (IBM, Microsoft and Rigetti Computing) and pharma research outfits (SRI International, AstraZeneca) to explore QC’s potential in modeling protein. By making it easier to model the behavior of proteins, quantum computing can help researchers understand existing drugs and create new drugs to treat diseases like Alzheimer’s and cancer.\", \"retrieve\": \"Summary: Quantum computing, leveraging the principles of quantum mechanics, has been\\\\nfound to significantly enhance computational capabilities in principle, in some\\\\ncases beyond classical computing limits. This paper explores quantum\\\\ncomputing\\'s potential to address complex, large-scale problems in\\\\ntransportation systems. It focuses on three principal paradigms: Gate-based\\\\nquantum computing, Quantum annealing, and Quantum machine learning, which,\\\\nthough based on gate-based quantum computing, is treated as distinct due to its\\\\nunique methods and applications. Each paradigm’s foundational concepts,\\\\npractical applications, and potential impacts on the field are discussed to\\\\nprovide a comprehensive overview of quantum computing strategies and their\\\\nfuture implications.\\\\n\\\\n Source: http://arxiv.org/abs/2503.21302v1\"}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = research.invoke(\n",
    "    {'query':\"what is computer vision\"}\n",
    ")\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546eecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8a703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8f0ab448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def conductResearch(state:AgentState):\n",
    "#     print(\"HERE1\")\n",
    "    response = subagent.invoke({\n",
    "        \"query\":state['input']\n",
    "    })\n",
    "#     print(\"HERE2\")\n",
    "    try:\n",
    "        webSearch = json.loads(response['messages'][-1].content)['webSearch']\n",
    "        retrievedDocuments = json.loads(response['messages'][-1].content)['retrieve']\n",
    "    except:\n",
    "        print(response['messages'][-1].content)\n",
    "        return {'webSearch':response['messages'][-1].content,'retrievedDocuments':response['messages'][-1].content,'nextNode':'respond'}\n",
    "    return {'webSearch':webSearch,'retrievedDocuments':retrievedDocuments,'nextNode':'respond'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "68d72af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general(state:AgentState):\n",
    "    \n",
    "    chain = generalPrompt | llm\n",
    "    result = chain.invoke({\n",
    "        \"input\":state['input'],\n",
    "        \"chatHistory\":state['chatHistory'],\n",
    "        \"additionalInfo\":state['additionalInfo']\n",
    "    })\n",
    "#     print(\"GENERAL\",result.content)\n",
    "    return {'response':result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ceb2d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def moreInfo(state:AgentState):\n",
    "    \n",
    "    chain = moreInfoPrompt | llm\n",
    "    result = chain.invoke({\n",
    "        \"input\":state['input'],\n",
    "        \"chatHistory\":state['chatHistory'],\n",
    "        \"additionalInfo\":state['additionalInfo']\n",
    "    })\n",
    "#     print(\"MOREINFO:\",result.content)\n",
    "#     return {\"response\":result.content}\n",
    "#     try:\n",
    "#         queries = json.loads(result.content)['queries']\n",
    "#         return {\"response\": \"\\n\".join(queries)}\n",
    "#     except:\n",
    "#         return {\"response\": result.content}\n",
    "    try:\n",
    "        queries = json.loads(result.content)['queries']\n",
    "        \n",
    "    except:\n",
    "        print(result.content)\n",
    "        queries = result.content\n",
    "    \n",
    "    print(\"\\nThe query is too vague.Answer these questions for better understanding:\\n\",queries)    \n",
    "    additionalInfo = str(input(\"Enter:\"))\n",
    "    \n",
    "    return {\"additionalInfo\":additionalInfo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a2b462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "respondResearch = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"Generate research report based on this research plan and retieved data:{researchPlan}\n",
    "    Web Search: {webSearch}\n",
    "    Retrieved Documents: {retirevedDocuments}\"\"\"),\n",
    "    MessagesPlaceholder('chatHistory'),\n",
    "    (\"human\",\"{input}\"+\". \"+\"{additionalInfo}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "661be57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(state:AgentState):\n",
    "#     print(\"RESPOND\")\n",
    "    if \"researchResult\" in state:\n",
    "        return {\"response\": f\"Research Findings:\\n {state['researchResults']}\"}\n",
    "    else:\n",
    "        return {}\n",
    "#     print(\"RESPOND\",response)\n",
    "#     return {\"response\":response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7ad04dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(state:AgentState):\n",
    "    chain = respondResearch | llm\n",
    "    result = chain.invoke({\n",
    "        \"input\":state['input'],\n",
    "        \"chatHistory\":state['chatHistory'],\n",
    "        \"additionalInfo\":state['additionalInfo'],\n",
    "        \"webSearch\":state['webSearch'],\n",
    "        \"retirevedDocuments\":state['retrievedDocuments'],\n",
    "        \"researchPlan\":state['researchPlan']\n",
    "    })\n",
    "    return {'response':f\"Research Result:\\n {result.content}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "43e57746",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fde27307",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph.add_node(\"Router\",router)\n",
    "graph.add_node(\"createResearchPlan\",researchPlan)\n",
    "graph.add_node(\"conductResearch\",conductResearch)\n",
    "graph.add_node(\"general\",general)\n",
    "graph.add_node(\"moreInfo\",moreInfo)\n",
    "graph.add_node(\"respond\",respond)\n",
    "\n",
    "graph.set_entry_point(\"Router\")\n",
    "\n",
    "# graph.add_edge(\"Router\",\"createResearchPlan\")\n",
    "# graph.add_edge(\"Router\",\"general\")\n",
    "graph.add_edge(\"createResearchPlan\",\"conductResearch\")\n",
    "graph.add_edge(\"conductResearch\",\"respond\")\n",
    "# graph.add_edge(\"Router\",\"moreInfo\")\n",
    "graph.add_edge(\"respond\",END)\n",
    "graph.add_edge(\"general\",END)\n",
    "graph.add_edge(\"moreInfo\",\"Router\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"Router\",\n",
    "    lambda state: state['nextNode'],\n",
    "    {\n",
    "        \"general\":\"general\",\n",
    "        \"createResearchPlan\":\"createResearchPlan\",\n",
    "        \"moreInfo\":\"moreInfo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8606d60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(Image(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/langchain_core/runnables/graph.py:685\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_mermaid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    679\u001b[0m mermaid_syntax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    680\u001b[0m     curve_style\u001b[38;5;241m=\u001b[39mcurve_style,\n\u001b[1;32m    681\u001b[0m     node_colors\u001b[38;5;241m=\u001b[39mnode_colors,\n\u001b[1;32m    682\u001b[0m     wrap_label_n_words\u001b[38;5;241m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    683\u001b[0m     frontmatter_config\u001b[38;5;241m=\u001b[39mfrontmatter_config,\n\u001b[1;32m    684\u001b[0m )\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/langchain_core/runnables/graph_mermaid.py:293\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    287\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    288\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    289\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    290\u001b[0m         )\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[0;32m--> 293\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     supported_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/langchain_core/runnables/graph_mermaid.py:430\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m requests\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m    432\u001b[0m             img_bytes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/site-packages/urllib3/connectionpool.py:463\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac5756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b7e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fa0db8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    chatHistory = []\n",
    "    print(\"\\nAgent Response:\")\n",
    "    print(response[\"response\"])\n",
    "    chatHistory.extend([\n",
    "        HumanMessage(content=response[\"input\"]),\n",
    "        HumanMessage(content=response[\"response\"])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82421cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bf594556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Query:help me with my homework\n",
      "------------------------------------\n",
      "DECISION= clarify\n",
      "------------------------------------\n",
      "MOREINFO\n",
      "\n",
      "The query is too vague.Answer these questions for better understanding:\n",
      " ['Could you describe the topic of your homework so I can better assist you?', 'What kind of subject or subject is your homework about?', \"Do you have any specific questions or areas where you're getting stuck?\"]\n",
      "Enter:mathematics. what is 2+4\n",
      "------------------------------------\n",
      "DECISION= general\n",
      "\n",
      "i'll do my best to help with the math problem. the answer to 2 + 4 is 6. if you have any other math problems or need further assistance, feel free to ask.\n",
      "------------------------------------\n",
      "GENERAL\n",
      "\n",
      "Agent Response:\n",
      "The answer to 2 + 4 is 6.\n"
     ]
    }
   ],
   "source": [
    "inpt = str(input(\"Enter your Query:\"))\n",
    "research_result = agent.invoke({\n",
    "    \"input\":inpt,\n",
    "    \"chatHistory\":[],\n",
    "    \"nextNode\":None,\n",
    "    \"additionalInfo\":None\n",
    "})\n",
    "print_response(research_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "55685bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "DECISION= clarify\n",
      "------------------------------------\n",
      "MOREINFO\n",
      "\n",
      "The query is too vague.Answer these questions for better understanding:\n",
      " ['What are the different types of deep learning models used for classification?', 'Can you provide some examples of user action classification tasks?', 'How do I preprocess user action data for classification using deep learning?']\n",
      "Enter:user actions like walking, running,etc\n",
      "------------------------------------\n",
      "DECISION= to classify user actions like walking, running, etc., using deep learning, we can use convolutional neural networks (cnns) or recurrent neural networks (rnns) depending on the type of input data. here's a general approach:\n",
      "\n",
      "**input data:**\n",
      "\n",
      "* we'll use video sequences or image sequences as input data.\n",
      "* pre-process the data by:\n",
      "  - resizing the images to a fixed size (e.g., 224x224).\n",
      "  - normalizing the pixel values to the range [0, 1].\n",
      "  - converting the videos to image sequences (frames).\n",
      "\n",
      "**model architecture:**\n",
      "\n",
      "for **image-based classification**:\n",
      "\n",
      "1. **convolutional neural network (cnn)**:\n",
      "   - use a pre-trained cnn model like vgg16 or resnet50 as the feature extractor.\n",
      "   - freeze the weights of the pre-trained model and add a new classification layer on top.\n",
      "   - train the new classification layer on your dataset.\n",
      "\n",
      "for **video-based classification**:\n",
      "\n",
      "1. **recurrent neural network (rnn)** with **convolutional layers (cnn)**:\n",
      "   - use a combination of cnn and rnn layers to extract features from video frames.\n",
      "   - use a cnn to extract features from each frame, and then pass the features through an rnn to capture temporal dependencies.\n",
      "   - train the model using a video-level classification loss function.\n",
      "\n",
      "**model training:**\n",
      "\n",
      "1. **data preparation**: collect a large dataset of video/image sequences with corresponding labels (e.g., walking, running, etc.).\n",
      "2. **data augmentation**: apply random transformations (e.g., rotation, scaling, flipping) to the data to increase its diversity and prevent overfitting.\n",
      "3. **model training**: train the model using a deep learning framework (e.g., tensorflow, pytorch) with a suitable optimizer (e.g., adam, sgd) and a classification loss function (e.g., categorical cross-entropy).\n",
      "\n",
      "**model evaluation:**\n",
      "\n",
      "1. **metrics**: evaluate the model using metrics like accuracy, precision, recall, f1-score, and mean average precision (map).\n",
      "2. **confusion matrix**: create a confusion matrix to visualize the classification results and identify any misclassifications.\n",
      "\n",
      "here's some sample python code using pytorch for image-based classification:\n",
      "```python\n",
      "import torch\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "# define the model architecture\n",
      "class actionclassifier(nn.module):\n",
      "    def __init__(self):\n",
      "        super(actionclassifier, self).__init__()\n",
      "        self.cnn = torchvision.models.resnet50(pretrained=true)\n",
      "        self.fc = nn.linear(2048, 10)  # 10 action classes\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.cnn(x)\n",
      "        x = x.view(-1, 2048)\n",
      "        x = self.fc(x)\n",
      "        return x\n",
      "\n",
      "# load the dataset and create data loaders\n",
      "transform = transforms.compose([\n",
      "    transforms.resize(224),\n",
      "    transforms.centercrop(224),\n",
      "    transforms.totensor(),\n",
      "    transforms.normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "])\n",
      "\n",
      "dataset = torchvision.datasets.imagefolder(root='path/to/dataset', transform=transform)\n",
      "data_loader = torch.utils.data.dataloader(dataset, batch_size=32, shuffle=true)\n",
      "\n",
      "# initialize the model, optimizer, and loss function\n",
      "model = actionclassifier()\n",
      "criterion = nn.crossentropyloss()\n",
      "optimizer = torch.optim.adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# train the model\n",
      "for epoch in range(10):\n",
      "    for batch in data_loader:\n",
      "        inputs, labels = batch\n",
      "        outputs = model(inputs)\n",
      "        loss = criterion(outputs, labels)\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        print(f'epoch {epoch+1}, loss: {loss.item()}')\n",
      "```\n",
      "note: this is a simplified example, and you may need to adjust the model architecture, training parameters, and data preprocessing steps according to your specific use case.\n",
      "------------------------------------\n",
      "GENERAL\n",
      "\n",
      "Agent Response:\n",
      "**Classification of User Actions using Deep Learning**\n",
      "\n",
      "**Problem Statement:**\n",
      "Classify user actions such as walking, running, jumping, etc. using deep learning techniques.\n",
      "\n",
      "**Dataset:**\n",
      "We will use the following datasets:\n",
      "\n",
      "* **UCI-HAR Dataset**: This dataset contains 561 features, including acceleration and gyroscope readings, from a wearable device. It has 30 volunteers performing six activities: walking, walking upstairs, walking downstairs, sitting, standing, and lying.\n",
      "* **KINEMATICS Dataset**: This dataset contains 3D kinematics data from a wearable device. It has 15 volunteers performing six activities: walking, running, jumping, cycling, dancing, and standing.\n",
      "\n",
      "**Deep Learning Model:**\n",
      "\n",
      "We will use a Convolutional Neural Network (CNN) with a Time-Distributed layer to classify user actions.\n",
      "\n",
      "```python\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, TimeDistributed\n",
      "from keras.layers import Dropout, LSTM\n",
      "from keras.callbacks import EarlyStopping\n",
      "from keras.utils import to_categorical\n",
      "from keras.optimizers import Adam\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
      "\n",
      "# Load the UCI-HAR dataset\n",
      "from sklearn.datasets import fetch_kddcup99\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Load the KINEMATICS dataset\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# Preprocess the data\n",
      "def preprocess_data(X):\n",
      "    # Normalize the data\n",
      "    scaler = StandardScaler()\n",
      "    X_scaled = scaler.fit_transform(X)\n",
      "    \n",
      "    # Reshape the data\n",
      "    X_reshaped = X_scaled.reshape(-1, 1, 1, 60)\n",
      "    \n",
      "    return X_reshaped\n",
      "\n",
      "# Define the CNN model\n",
      "def define_model(input_shape):\n",
      "    model = Sequential()\n",
      "    model.add(TimeDistributed(Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape)))\n",
      "    model.add(TimeDistributed(MaxPooling3D((2, 2, 2))))\n",
      "    model.add(TimeDistributed(Flatten()))\n",
      "    model.add(LSTM(64, return_sequences=True))\n",
      "    model.add(LSTM(64))\n",
      "    model.add(Dense(64, activation='relu'))\n",
      "    model.add(Dropout(0.5))\n",
      "    model.add(Dense(6, activation='softmax'))\n",
      "    \n",
      "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
      "    \n",
      "    return model\n",
      "\n",
      "# Train the model\n",
      "def train_model(X_train, y_train, X_val, y_val):\n",
      "    model = define_model(X_train.shape[1:])\n",
      "    earlyStopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)\n",
      "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=[earlyStopping])\n",
      "    \n",
      "    return model, history\n",
      "\n",
      "# Evaluate the model\n",
      "def evaluate_model(model, X_test, y_test):\n",
      "    y_pred = model.predict(X_test)\n",
      "    y_pred_class = y_pred.argmax(axis=-1)\n",
      "    y_test_class = y_test.argmax(axis=-1)\n",
      "    \n",
      "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
      "    report = classification_report(y_test_class, y_pred_class)\n",
      "    conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
      "    \n",
      "    return accuracy, report, conf_matrix\n",
      "\n",
      "# Main function\n",
      "if __name__ == \"__main__\":\n",
      "    # Load the UCI-HAR dataset\n",
      "    X_train, X_test, y_train, y_test = train_test_split(preprocess_data(X_train), to_categorical(y_train), test_size=0.2, random_state=42)\n",
      "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
      "    \n",
      "    # Train the model\n",
      "    model, history = train_model(X_train, y_train, X_val, y_val)\n",
      "    \n",
      "    # Evaluate the model\n",
      "    accuracy, report, conf_matrix = evaluate_model(model, X_test, y_test)\n",
      "    \n",
      "    print(\"Accuracy:\", accuracy)\n",
      "    print(\"Classification Report:\")\n",
      "    print(report)\n",
      "    print(\"Confusion Matrix:\")\n",
      "    print(conf_matrix)\n",
      "```\n",
      "\n",
      "**Results:**\n",
      "\n",
      "* **Accuracy:** 95.6%\n",
      "* **Classification Report:**\n",
      "```\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      100\n",
      "           1       0.92      0.92      0.92       90\n",
      "           2       0.91      0.91      0.91       90\n",
      "           3       0.93      0.93      0.93       90\n",
      "           4       0.94      0.94      0.94       90\n",
      "           5       0.96      0.96      0.96      100\n",
      "\n",
      "    accuracy                           0.96     560\n",
      "   macro avg       0.94      0.94      0.94     560\n",
      "weighted avg       0.96      0.96      0.96     560\n",
      "```\n",
      "* **Confusion Matrix:**\n",
      "```\n",
      "[[ 95   5   0   0   0   0]\n",
      " [  0  92   2   0   0   0]\n",
      " [  0   0  91   1   0   0]\n",
      " [  0   0   0  93   1   0]\n",
      " [  0   0   0   0  94   0]\n",
      " [  0   0   0   0   0  96]]\n",
      "```\n",
      "\n",
      "**Discussion:**\n",
      "\n",
      "The proposed CNN model achieves an accuracy of 95.6% on the UCI-HAR dataset and 96.0% on the KINEMATICS dataset. The classification report and confusion matrix show that the model performs well on all classes, with the highest accuracy on the \"walking\" and \"running\" classes. The model also performs well on the \"jumping\" and \"cycling\" classes, but has a slightly lower accuracy on the \"dancing\" and \"standing\" classes.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The proposed CNN model is effective in classifying user actions using deep learning techniques. The model achieves high accuracy on both datasets and performs well on all classes. The results show that the model is suitable for real-world applications, such as activity recognition and health monitoring.\n"
     ]
    }
   ],
   "source": [
    "research_result = agent.invoke({\n",
    "        \"input\": \"Classification of user actions using deep learning\",\n",
    "        \"chatHistory\": [],\n",
    "        \"nextNode\": None,\n",
    "        \"additionalInfo\":None\n",
    "})\n",
    "# print_response(research_result)\n",
    "print_response(research_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3b28db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Query:what is deep learning? write a report on it\n",
      "------------------------------------\n",
      "DECISION= research\n",
      "------------------------------------\n",
      "RESEARCH\n",
      "\n",
      "Agent Response:\n",
      "Research Result:\n",
      " **Deep Learning Report**\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Deep learning is a subset of machine learning that involves the use of artificial neural networks to analyze and interpret data. It is a type of machine learning that is inspired by the structure and function of the human brain, where layers of interconnected nodes (neurons) process and transmit information. Deep learning has gained significant attention in recent years due to its high accuracy and efficiency in solving complex problems.\n",
      "\n",
      "**What is Deep Learning?**\n",
      "\n",
      "Deep learning is a type of machine learning that uses neural networks with multiple layers to learn complex patterns and relationships in data. These neural networks are composed of multiple layers of nodes or \"neurons,\" each of which applies a non-linear transformation to the input data. The output from each layer is then passed to the next layer, allowing the network to learn complex and abstract representations of the data.\n",
      "\n",
      "**Types of Deep Learning**\n",
      "\n",
      "There are several types of deep learning, including:\n",
      "\n",
      "1. **Supervised Learning**: This type of deep learning involves training a neural network on labeled data, where the network learns to predict the output based on the input.\n",
      "2. **Unsupervised Learning**: This type of deep learning involves training a neural network on unlabeled data, where the network learns to identify patterns and relationships.\n",
      "3. **Reinforcement Learning**: This type of deep learning involves training a neural network to make decisions based on rewards or penalties.\n",
      "4. **Generative Learning**: This type of deep learning involves training a neural network to generate new data that is similar to the existing data.\n",
      "\n",
      "**Key Components of Deep Learning**\n",
      "\n",
      "The key components of deep learning include:\n",
      "\n",
      "1. **Artificial Neural Networks**: These are networks of interconnected nodes (neurons) that process and transmit information.\n",
      "2. **Activation Functions**: These are mathematical functions that introduce non-linearity into the network, allowing it to learn complex relationships.\n",
      "3. **Backpropagation**: This is an algorithm used to train neural networks by minimizing the error between the predicted output and the actual output.\n",
      "4. **Convolutional Neural Networks (CNNs)**: These are a type of neural network that is particularly effective for image and video processing.\n",
      "\n",
      "**Applications of Deep Learning**\n",
      "\n",
      "Deep learning has a wide range of applications, including:\n",
      "\n",
      "1. **Image Recognition**: Deep learning is used in self-driving cars, facial recognition, and medical image analysis.\n",
      "2. **Natural Language Processing (NLP)**: Deep learning is used in language translation, text summarization, and language generation.\n",
      "3. **Speech Recognition**: Deep learning is used in speech recognition, voice assistants, and voice-controlled systems.\n",
      "4. **Recommendation Systems**: Deep learning is used in personalized recommendations, product recommendation, and content recommendation.\n",
      "\n",
      "**Advantages of Deep Learning**\n",
      "\n",
      "The advantages of deep learning include:\n",
      "\n",
      "1. **High Accuracy**: Deep learning can achieve high accuracy in solving complex problems.\n",
      "2. **Efficiency**: Deep learning can process large amounts of data quickly and efficiently.\n",
      "3. **Flexibility**: Deep learning can be applied to a wide range of problems, including image, speech, and text processing.\n",
      "\n",
      "**Challenges of Deep Learning**\n",
      "\n",
      "The challenges of deep learning include:\n",
      "\n",
      "1. **Training Time**: Deep learning requires large amounts of data and computing power, which can be time-consuming.\n",
      "2. **Overfitting**: Deep learning can suffer from overfitting, where the network becomes too complex and fails to generalize to new data.\n",
      "3. **Interpretability**: Deep learning can be difficult to interpret, making it challenging to understand how the network arrived at its predictions.\n",
      "\n",
      "**Future Prospects of Deep Learning**\n",
      "\n",
      "The future of deep learning looks promising, with several areas of research and development expected to make significant progress in the coming years, including:\n",
      "\n",
      "1. **Explainability**: Researchers are working on developing techniques to explain and interpret deep learning models.\n",
      "2. **Adversarial Robustness**: Researchers are working on developing techniques to make deep learning models more robust to adversarial attacks.\n",
      "3. **Transfer Learning**: Researchers are working on developing techniques to transfer knowledge from one task to another.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, deep learning is a powerful tool for analyzing and interpreting complex data. Its applications are diverse and widespread, and its advantages include high accuracy, efficiency, and flexibility. However, it also faces several challenges, including training time, overfitting, and interpretability. As research and development continue to advance, deep learning is expected to play an increasingly important role in many areas of science and technology.\n",
      "\n",
      "**References**\n",
      "\n",
      "1. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.\n",
      "2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.\n",
      "3. Schmidhuber, J. (2015). Deep learning in natural language processing. Journal of Machine Learning Research, 16, 1-26.\n",
      "4. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1-9).\n"
     ]
    }
   ],
   "source": [
    "inpt = str(input(\"Enter your Query:\"))\n",
    "research_result = agent.invoke({\n",
    "    \"input\":inpt,\n",
    "    \"chatHistory\":[],\n",
    "    \"nextNode\":None,\n",
    "    \"additionalInfo\":None\n",
    "})\n",
    "print_response(research_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fc2beb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b0c153fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Query:generate a report on machine learning\n",
      "------------------------------------\n",
      "DECISION= research\n",
      "------------------------------------\n",
      "RESEARCH\n",
      "\n",
      "Agent Response:\n",
      "Research Result:\n",
      " **Comprehensive Report on Machine Learning**\n",
      "\n",
      "**Executive Summary**\n",
      "\n",
      "Machine learning is a subfield of artificial intelligence that involves the use of algorithms and statistical models to enable machines to learn from data, without being explicitly programmed. This report provides an in-depth understanding of machine learning, its history, types, applications, challenges, and future directions.\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Machine learning has revolutionized the way we approach problems in various fields, including computer vision, natural language processing, and predictive analytics. The ability of machines to learn from data has enabled them to make predictions, classify objects, and identify patterns, leading to significant improvements in efficiency and accuracy.\n",
      "\n",
      "**History of Machine Learning**\n",
      "\n",
      "Machine learning has its roots in the 1950s, when computer scientists began exploring the idea of programming computers to learn from data. The term \"machine learning\" was first coined in 1959 by Arthur Samuel, who developed a computer program that could play checkers. However, it wasn't until the 1990s that machine learning began to gain significant attention, with the development of neural networks and support vector machines.\n",
      "\n",
      "**Types of Machine Learning**\n",
      "\n",
      "There are three primary types of machine learning:\n",
      "\n",
      "1. **Supervised Learning**: In this type of learning, the machine is trained on labeled data, where the correct output is already known. The goal is to learn a mapping between input data and output labels.\n",
      "2. **Unsupervised Learning**: In this type of learning, the machine is trained on unlabeled data, and the goal is to identify patterns or relationships in the data.\n",
      "3. **Reinforcement Learning**: In this type of learning, the machine learns by interacting with an environment and receiving rewards or penalties for its actions.\n",
      "\n",
      "**Applications of Machine Learning**\n",
      "\n",
      "Machine learning has numerous applications in various fields, including:\n",
      "\n",
      "1. **Computer Vision**: Machine learning is used in image recognition, object detection, and image segmentation.\n",
      "2. **Natural Language Processing**: Machine learning is used in text classification, sentiment analysis, and language translation.\n",
      "3. **Predictive Analytics**: Machine learning is used in forecasting, recommendation systems, and risk assessment.\n",
      "4. **Healthcare**: Machine learning is used in disease diagnosis, personalized medicine, and medical imaging.\n",
      "5. **Finance**: Machine learning is used in credit scoring, risk assessment, and portfolio optimization.\n",
      "\n",
      "**Challenges of Machine Learning**\n",
      "\n",
      "Despite its numerous applications, machine learning faces several challenges, including:\n",
      "\n",
      "1. **Data Quality**: Machine learning requires high-quality data to learn from, but data is often noisy, incomplete, or biased.\n",
      "2. **Overfitting**: Machine learning models can overfit the training data, leading to poor performance on new, unseen data.\n",
      "3. **Interpretability**: Machine learning models can be difficult to interpret, making it challenging to understand why a particular decision was made.\n",
      "4. **Explainability**: Machine learning models can be opaque, making it challenging to explain why a particular decision was made.\n",
      "\n",
      "**Future Directions of Machine Learning**\n",
      "\n",
      "Machine learning is a rapidly evolving field, with new techniques and applications emerging regularly. Some of the future directions of machine learning include:\n",
      "\n",
      "1. **Explainable AI**: Developing machine learning models that are transparent and explainable.\n",
      "2. **Transfer Learning**: Developing machine learning models that can learn from one task and apply to another.\n",
      "3. **Adversarial Training**: Developing machine learning models that are robust to adversarial attacks.\n",
      "4. **Quantum Machine Learning**: Developing machine learning models that can take advantage of quantum computing.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Machine learning has revolutionized the way we approach problems in various fields, and its applications are vast and varied. However, machine learning also faces several challenges, including data quality, overfitting, interpretability, and explainability. As machine learning continues to evolve, it is essential to address these challenges and develop new techniques and applications to make machine learning more robust and transparent.\n",
      "\n",
      "**Recommendations**\n",
      "\n",
      "Based on this report, we recommend the following:\n",
      "\n",
      "1. **Invest in machine learning research and development**: To address the challenges of machine learning and develop new techniques and applications.\n",
      "2. **Develop explainable AI models**: To make machine learning models more transparent and interpretable.\n",
      "3. **Invest in data quality**: To ensure that machine learning models are trained on high-quality data.\n",
      "4. **Develop transfer learning techniques**: To enable machine learning models to learn from one task and apply to another.\n",
      "\n",
      "**Limitations**\n",
      "\n",
      "This report is limited by the following:\n",
      "\n",
      "1. **Time constraints**: The report was completed within a limited timeframe.\n",
      "2. **Access to experts**: The report may not have included the input of experts in the field of machine learning.\n",
      "3. **Data quality**: The report may not have included the most up-to-date or accurate data.\n",
      "\n",
      "**Future Research Directions**\n",
      "\n",
      "Based on this report, we recommend the following future research directions:\n",
      "\n",
      "1. **Explainable AI**: Developing machine learning models that are transparent and explainable.\n",
      "2. **Transfer Learning**: Developing machine learning models that can learn from one task and apply to another.\n",
      "3. **Adversarial Training**: Developing machine learning models that are robust to adversarial attacks.\n",
      "4. **Quantum Machine Learning**: Developing machine learning models that can take advantage of quantum computing.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, machine learning is a rapidly evolving field with numerous applications and challenges. This report provides an in-depth understanding of machine learning, its history, types, applications, challenges, and future directions. We recommend investing in machine learning research and development, developing explainable AI models, investing in data quality, and developing transfer learning techniques to make machine learning more robust and transparent.\n"
     ]
    }
   ],
   "source": [
    "inpt = str(input(\"Enter your Query:\"))\n",
    "research_result = agent.invoke({\n",
    "    \"input\":inpt,\n",
    "    \"chatHistory\":[],\n",
    "    \"nextNode\":None,\n",
    "    \"additionalInfo\":None\n",
    "})\n",
    "print_response(research_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfef08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
